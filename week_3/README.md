# Language Modeling and N-grams

- **Language modeling:** The task of predicting the next word from a linguistic context

- **N-gram:** A collection of N-lenght words sequence

## N-grams as state of the art in Language Modeling
A study was conducted to determine whether the linguistic sophistication may be used to improve the performance of language models. 

- N-gram language models capture constrains of the language by conditioning the probability of a word on a fixed small number of predecesors.
- Markov models were deemed insufficient to capture the complexity of language.
- Using different sources of rich linguistic information helps to improve speech recognition if the phenomenna is prevalent and the instances involve errors from the speech recognizer.
- One of the first attemps at Linguistically based modeling was context-free grammars.
- The experimentation conducted involved humans to rate the ten-best list hypotheses generated by a speech recognizer. Some finds include:
  - Humans with the choice to edit the hypotheses, they were able to improve the accuracy of the recognizer significantly.
  - Humans with no choice to edit the hypotheses underperformed their counterparts, explaining that the strategy used was to discard the less likely hypotheses and choosing the best fit even when they were not correct.
  - Humans underperformed the recognizer when the contextual language was more sophisticated and advanced than from other context sources analyzed.

The paper gives some light on a problem that seemed AI-complete, as it is first needed to solve linguistics befor jumping into algorithms. The paper also gives some insight on the importance of the human factor in the process of speech recognition and how it may help to improve the performance of the algorithms.

## N-gram Language Modeling

- N-gram language models are a simple but effective approach to language modeling.
- Trainning of other language models such as neural networks is not only more complex but computationally expensive. 
- N-gram language models rely on a few hash lookups, outperforming query times of neural networks.
- While N-grams cannot reach the peak performance of neural networks, particularly decreasing perplexity by 50%, they come with a 10,000 times speedup in trainning time.
- They also perform better than neural networks in low-resource settings where morphological rich languages and large portions of Out-of-Vocabulary words are present.
- There are several instances of N-gram language modeling techniques, such as:
  - **Kneser-Ney Smoothing:** A technique that uses a discounting method to estimate the probability of a word based on the context.
  - **Modified Kneser-Ney Smoothing:** A technique that uses a discounting method to estimate the probability of a word based on the context, but with a different discounting method that is more sensitive to existing mass.
  - **Generalized Kneser-Ney Smoothing:** A technique allows richer parameterization of the discounting method, allowing the model to have more flexibility when capturing the desired distribution.

From these techniques, BKN is the best performing one in a resource scarce setting, not only taking less time to train but also performing better in perplexity, specially in morphologically rich languages with large portions of OOV words.

## Experiment 1 results

Running the experiment generates text that may seem like a real sentence, but it is not. The reason is that the model is trained on a small corpus, and the generated text is either not undestandable or simply repeats the phrases from the input. Also, from the lectures I learned that increasing the number of n-grams does not necessarily improve the performance of the model, as it may overfit the data, and since the corpus is small, that is the case in this experiment.

## Experiment 2 results

This experiment used a larger corpus, and the results are better than the previous experiment. The generated text is more rich and the model is able to generate new sentences that are not in the corpus. However, the model is still not able to maintain cohesion within different sentences. This is because the corpus is not large enough to capture the complexity of the language. Something that is posibble to notices is that the generated text has a lot of symbols, this may be due to the fact that the corpus is not cleaned, and it contains a lot of symbols that are not words.